{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment for colab\n",
    "## upload datasetGenerator.py for preprocessing dataset\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# ! unzip -q \"/content/drive/MyDrive/Colab Notebooks/BVP.zip\"\n",
    "# ! python /content/datasetGenerator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_for_test = 0.2\n",
    "num_class = 3\n",
    "ALL_MOTION = [i for i in range(1, num_class+3)]\n",
    "N_MOTION = len(ALL_MOTION) # Number of output classes\n",
    "T_MAX = 38 # Number of timestamps\n",
    "n_gru_hidden_units = 128\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModule, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=2, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8 * 10 * 10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "class ConvGRUModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, num_timestamps):\n",
    "        super(ConvGRUModel, self).__init__()\n",
    "        \n",
    "        # CNN module for each input timestamp\n",
    "        self.cnn_modules = nn.ModuleList([\n",
    "            CNNModule() for _ in range(num_timestamps)\n",
    "        ])\n",
    "        \n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(32, hidden_size, num_layers=num_timestamps, batch_first=True, dropout=0.25)\n",
    "\n",
    "        # Fully connected layer at the output of last GRU\n",
    "        self.fc_out = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # Relu activation for fully connected\n",
    "        self.relu = nn.ReLU()\n",
    "        # Softmax activation for classification\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply CNN module sequentially for each timestamp\n",
    "        x = np.swapaxes(x, 0, 1)\n",
    "        x = [module(xi) for module, xi in zip(self.cnn_modules, x)]\n",
    "        x = torch.stack(x, dim=1)  # Stack along the time dimension\n",
    "        \n",
    "        # GRU layer\n",
    "        x, _ = self.gru(x)\n",
    "\n",
    "        # Apply ReLU activation after the GRU layer\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Fully connected layer at the output of last GRU\n",
    "        x = self.fc_out(x[:, -1, :])\n",
    "        \n",
    "        # Softmax for classification\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "num_clients = 5\n",
    "batch_size = 8\n",
    "client_datasets = {}\n",
    "client_loaders = {}\n",
    "\n",
    "for i in range(1, num_clients + 1):\n",
    "    # Load client data\n",
    "    client_data = torch.load(f'./data/data{i}.pt')\n",
    "    data = torch.from_numpy(client_data['data']).float()\n",
    "    label = torch.from_numpy(client_data['label']).long()\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    data_train, data_test, label_train, label_test = train_test_split(\n",
    "        data, label, test_size=fraction_for_test, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = TensorDataset(data_train, label_train)\n",
    "    test_dataset = TensorDataset(data_test, label_test)\n",
    "    client_datasets[f'client{i}'] = {'train': train_dataset, 'test':test_dataset}\n",
    "\n",
    "    # Set up data loaders for each client's\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    client_loaders[f'client{i}'] = {'train': train_loader, 'test':test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvgAlgorithm:\n",
    "    def __init__(self, model, n_gru_hidden_units, num_class, T_MAX, train_loader):\n",
    "        self.model = model\n",
    "        self.global_model = model(n_gru_hidden_units, num_class, T_MAX)\n",
    "        self.train_loader = train_loader\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def train(self, model, device, train_loader, optimizer, criterion):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        return train_loss, train_accuracy\n",
    "\n",
    "    def test(self, model, device, test_loader, criterion):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        return test_loss, test_accuracy\n",
    "\n",
    "    def run(self, num_rounds, num_epochs):\n",
    "        result = []\n",
    "        round_accuracy_all = []\n",
    "        for round in range(num_rounds):\n",
    "            print(f\"\\n---------- Round {round + 1}/{num_rounds} ----------\")\n",
    "\n",
    "            local_model_updates = []\n",
    "            client_results = {'loss':[], 'accuracy':[]}\n",
    "\n",
    "            for client_id in range(1, len(client_loaders)+1):\n",
    "                print(f\"\\nTraining on Client {client_id}\")\n",
    "\n",
    "                # Create a local copy\n",
    "                local_model = self.model(n_gru_hidden_units, num_class, T_MAX).to(self.device)\n",
    "                local_model.load_state_dict(self.global_model.state_dict())\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(local_model.parameters(), lr=0.001)\n",
    "\n",
    "                # Local training\n",
    "                loss, accuracy = [], []\n",
    "                for epoch in range(num_epochs):\n",
    "\n",
    "                    train_loss, train_accuracy = self.train(local_model, self.device, self.train_loader[f'client{client_id}']['train'], optimizer , criterion)\n",
    "                    val_loss, val_accuracy = self.test(local_model, self.device, self.train_loader[f'client{client_id}']['test'], criterion)\n",
    "\n",
    "                    loss.append((train_loss, val_loss))\n",
    "                    accuracy.append((train_accuracy, val_accuracy))\n",
    "                    print(f'        Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%,', \n",
    "                          f' Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "                    \n",
    "\n",
    "                client_results['loss'].append(loss)\n",
    "                client_results['accuracy'].append(accuracy)\n",
    "                local_model_updates.append(local_model.state_dict())\n",
    "                \n",
    "            # Weighted average\n",
    "            averaged_state_dict = {}\n",
    "            client_size = [len(v['train'].dataset) for k, v in self.train_loader.items()]\n",
    "            for key in self.global_model.state_dict():\n",
    "                averaged_state_dict[key] = sum(client_size[ind]*update[key] for ind, update in enumerate(local_model_updates)) / np.sum(client_size)\n",
    "            \n",
    "            # update global model\n",
    "            self.global_model.load_state_dict(averaged_state_dict)\n",
    "\n",
    "            # calculate round test accuracy with weighted mean\n",
    "            round_acc = []\n",
    "            client_size = [len(v['test'].dataset) for k, v in self.train_loader.items()]\n",
    "            for cl, data_ in self.train_loader.items():\n",
    "                _, val_accuracy = self.test(self.global_model, self.device, data_['test'], criterion)\n",
    "                round_acc.append(val_accuracy)\n",
    "            round_accuracy = np.average(round_acc, weights=client_size)\n",
    "            print(f'\\n The round accuracy is: {round_accuracy}')\n",
    "            round_accuracy_all.append(round_accuracy)\n",
    "            result.append(client_results)\n",
    "            \n",
    "        return round_accuracy_all, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- Round 1/5 ----------\n",
      "\n",
      "Training on Client 1\n",
      "        Epoch: 1, Train Loss: 1.5013, Train Accuracy: 32.43%,  Val Loss: 1.4904, Val Accuracy: 30.63%\n",
      "        Epoch: 2, Train Loss: 1.4920, Train Accuracy: 31.40%,  Val Loss: 1.4903, Val Accuracy: 35.86%\n",
      "        Epoch: 3, Train Loss: 1.4915, Train Accuracy: 32.88%,  Val Loss: 1.4984, Val Accuracy: 30.63%\n",
      "\n",
      "Training on Client 2\n",
      "        Epoch: 1, Train Loss: 1.4971, Train Accuracy: 34.28%,  Val Loss: 1.4831, Val Accuracy: 38.95%\n",
      "        Epoch: 2, Train Loss: 1.4874, Train Accuracy: 36.97%,  Val Loss: 1.5023, Val Accuracy: 30.32%\n",
      "        Epoch: 3, Train Loss: 1.4883, Train Accuracy: 34.12%,  Val Loss: 1.4810, Val Accuracy: 38.95%\n",
      "\n",
      "Training on Client 3\n",
      "        Epoch: 1, Train Loss: 1.4992, Train Accuracy: 33.42%,  Val Loss: 1.4968, Val Accuracy: 31.20%\n",
      "        Epoch: 2, Train Loss: 1.4945, Train Accuracy: 31.69%,  Val Loss: 1.4920, Val Accuracy: 31.20%\n",
      "        Epoch: 3, Train Loss: 1.4914, Train Accuracy: 32.49%,  Val Loss: 1.4989, Val Accuracy: 31.20%\n",
      "\n",
      "Training on Client 4\n",
      "        Epoch: 1, Train Loss: 1.4988, Train Accuracy: 33.40%,  Val Loss: 1.4889, Val Accuracy: 33.02%\n",
      "        Epoch: 2, Train Loss: 1.4910, Train Accuracy: 32.20%,  Val Loss: 1.5004, Val Accuracy: 33.02%\n",
      "        Epoch: 3, Train Loss: 1.4926, Train Accuracy: 33.02%,  Val Loss: 1.4923, Val Accuracy: 30.53%\n",
      "\n",
      "Training on Client 5\n",
      "        Epoch: 1, Train Loss: 1.4981, Train Accuracy: 32.47%,  Val Loss: 1.4909, Val Accuracy: 32.17%\n",
      "        Epoch: 2, Train Loss: 1.4924, Train Accuracy: 32.47%,  Val Loss: 1.4908, Val Accuracy: 34.00%\n",
      "        Epoch: 3, Train Loss: 1.4927, Train Accuracy: 32.47%,  Val Loss: 1.4897, Val Accuracy: 32.17%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FedAvgAlgorithm' object has no attribute 'data_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_22952/2726564382.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                     )\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mfed_avg_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfed_avg_algorithm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_22952/436009117.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, num_rounds, num_epochs)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mclient_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m                 \u001b[0mround_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mround_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclient_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FedAvgAlgorithm' object has no attribute 'data_'"
     ]
    }
   ],
   "source": [
    "fed_avg_algorithm = FedAvgAlgorithm(model=ConvGRUModel,\n",
    "                                    n_gru_hidden_units=n_gru_hidden_units, \n",
    "                                    num_class=N_MOTION, \n",
    "                                    T_MAX=T_MAX, \n",
    "                                    train_loader=client_loaders\n",
    "                                    )\n",
    "\n",
    "fed_avg_result = fed_avg_algorithm.run(num_rounds=5, num_epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
