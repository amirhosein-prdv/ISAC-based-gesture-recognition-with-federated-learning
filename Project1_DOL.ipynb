{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture = {'Push & Pull':1,\n",
    "            'Sweep':2,\n",
    "            'Clap':3,\n",
    "            'Slide':4,\n",
    "            'Draw-N (V)':5,\n",
    "            'Draw-O (V)':6,\n",
    "            'Draw-Rectangle':7,\n",
    "            'Draw-Triangle':8,\n",
    "            'Draw-Zigzag (H)':9,\n",
    "            'Draw-Zigzag (V)':10,\n",
    "            'Draw-N (H)':11,\n",
    "            'Draw-O (H)':12,\n",
    "            'Draw-1':13,\n",
    "            'Draw-2':14,\n",
    "            'Draw-3':15,\n",
    "            'Draw-4':16,\n",
    "            'Draw-5':17,\n",
    "            'Draw-6':18,\n",
    "            'Draw-7':19,\n",
    "            'Draw-8':20,\n",
    "            'Draw-9':21,\n",
    "            'Draw-0':22,\n",
    "            }\n",
    "\n",
    "dataset_gesture = { '20181109':['Push & Pull', 'Sweep', 'Clap', 'Slide', 'Draw-Zigzag (V)', 'Draw-N (V)'],\n",
    "                    '20181112':['Draw-1', 'Draw-2', 'Draw-3', 'Draw-4', 'Draw-5', 'Draw-6', 'Draw-7', 'Draw-8', 'Draw-9', 'Draw-0'],\n",
    "                    '20181115':['Push & Pull', 'Sweep', 'Clap', 'Draw-O (V)', 'Draw-Zigzag (V)', 'Draw-N (V)'],\n",
    "                    '20181117':['Push & Pull', 'Sweep', 'Clap', 'Draw-O (V)', 'Draw-Zigzag (V)', 'Draw-N (V)'],\n",
    "                    '20181118':['Push & Pull', 'Sweep', 'Clap', 'Draw-O (V)', 'Draw-Zigzag (V)', 'Draw-N (V)'],\n",
    "                    '20181121':['Slide', 'Draw-O (H)', 'Draw-Zigzag (H)', 'Draw-N (H)', 'Draw-Triangle', 'Draw-Rectangle'],\n",
    "                    '20181127':['Slide', 'Draw-O (H)', 'Draw-Zigzag (H)', 'Draw-N (H)', 'Draw-Triangle', 'Draw-Rectangle'],\n",
    "                    '20181128':['Push & Pull', 'Sweep', 'Clap', 'Draw-O (H)', 'Draw-Zigzag (H)', 'Draw-N (H)'],\n",
    "                    '20181130':['Push & Pull', 'Sweep', 'Clap', 'Slide', 'Draw-O (H)', 'Draw-Zigzag (H)', 'Draw-N (H)', 'Draw-Triangle', 'Draw-Rectangle'],\n",
    "                    '20181204':['Push & Pull', 'Sweep', 'Clap', 'Slide', 'Draw-O (H)', 'Draw-Zigzag (H)', 'Draw-N (H)', 'Draw-Triangle', 'Draw-Rectangle'],\n",
    "                    '20181205':['Draw-O (H)', 'Draw-Zigzag (H)', 'Draw-N (H)', 'Draw-Triangle', 'Draw-Rectangle'],\n",
    "                    '20181206':['Slide', 'Draw-O (H)', 'Draw-Zigzag (H)', 'Draw-N (H)', 'Draw-Triangle', 'Draw-Rectangle'],\n",
    "                    '20181208':['Push & Pull', 'Sweep', 'Clap'],\n",
    "                    '20181209':['Push & Pull', 'Sweep', 'Clap', 'Slide', 'Draw-O (H)', 'Draw-Zigzag (H)'],\n",
    "                    '20181211':['Push & Pull', 'Sweep', 'Clap', 'Slide', 'Draw-O (H)', 'Draw-Zigzag (H)'],\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_existing_model = False\n",
    "fraction_for_test = 0.2\n",
    "data_dir = 'BVP/'\n",
    "ALL_MOTION = [i for i in range(1, 10)]\n",
    "N_MOTION = len(ALL_MOTION) # Number of output classes\n",
    "T_MAX = 0 # Number of timestamps\n",
    "n_gru_hidden_units = 128\n",
    "n_batch_size = 32\n",
    "f_learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoding(label, num_class):\n",
    "    # label(list)=>_label(ndarray): [N,]=>[N,num_class]\n",
    "    label = np.array(label).astype('int32')\n",
    "    # assert (np.arange(0,np.unique(label).size)==np.unique(label)).prod()    # Check label from 0 to N\n",
    "    label = np.squeeze(label)\n",
    "    _label = np.eye(num_class)[label-1]     # from label to onehot\n",
    "    return _label\n",
    "\n",
    "def normalize_data(data_1):\n",
    "    # data(ndarray)=>data_norm(ndarray): [20,20,T]=>[20,20,T]\n",
    "    data_1_max = np.concatenate((data_1.max(axis=0),data_1.max(axis=1)),axis=0).max(axis=0)\n",
    "    data_1_min = np.concatenate((data_1.min(axis=0),data_1.min(axis=1)),axis=0).min(axis=0)\n",
    "    if (len(np.where((data_1_max - data_1_min) == 0)[0]) > 0):\n",
    "        return data_1\n",
    "    data_1_max_rep = np.tile(data_1_max,(data_1.shape[0],data_1.shape[1],1))\n",
    "    data_1_min_rep = np.tile(data_1_min,(data_1.shape[0],data_1.shape[1],1))\n",
    "    data_1_norm = (data_1 - data_1_min_rep) / (data_1_max_rep - data_1_min_rep)\n",
    "    return  data_1_norm\n",
    "\n",
    "def zero_padding(data, T_MAX):\n",
    "    # data(list)=>data_pad(ndarray): [20,20,T1/T2/...]=>[20,20,T_MAX]\n",
    "    data_pad = []\n",
    "    for i in range(len(data)):\n",
    "        t = np.array(data[i]).shape[2]\n",
    "        data_pad.append(np.pad(data[i], ((0, 0),(0,0),(T_MAX - t,0)), 'constant', constant_values = 0).tolist())\n",
    "    return np.array(data_pad)\n",
    "\n",
    "def load_data(path_to_data, motion_sel, target_user):\n",
    "    global T_MAX\n",
    "    data = []\n",
    "    label = []\n",
    "    for data_root, data_dirs, data_files in os.walk(path_to_data):\n",
    "        for data_file_name in data_files:\n",
    "            file_path = os.path.join(data_root, data_file_name)\n",
    "            date = data_root.split('/')[1].split('-')[0]\n",
    "            try:\n",
    "                data_1 = scio.loadmat(file_path)['velocity_spectrum_ro']\n",
    "                ges_num = int(data_file_name.split('-')[1])\n",
    "                ges_lable = dataset_gesture[date][ges_num-1]\n",
    "                label_1 = gesture[ges_lable]\n",
    "                user = int(data_file_name.split('-')[0].split('user')[-1])\n",
    "                location = int(data_file_name.split('-')[2])\n",
    "                orientation = int(data_file_name.split('-')[3])\n",
    "                repetition = int(data_file_name.split('-')[4])\n",
    "                # print(data_1.shape)\n",
    "                \n",
    "                # Select Motion\n",
    "                if (label_1 not in motion_sel):\n",
    "                    continue\n",
    "                \n",
    "                # Select User\n",
    "                if user != target_user:\n",
    "                    continue\n",
    "                \n",
    "                # Select Location\n",
    "                # if (location not in [1,2,3,5]):\n",
    "                #     continue\n",
    "\n",
    "                # Select Orientation\n",
    "                # if (orientation not in [1,2,4,5]):\n",
    "                #     continue\n",
    "                \n",
    "                # Normalization\n",
    "                data_normed_1 = normalize_data(data_1)\n",
    "                \n",
    "                # Update T_MAX\n",
    "                if T_MAX < np.array(data_1).shape[2]:\n",
    "                    T_MAX = np.array(data_1).shape[2]                \n",
    "            except Exception as error:\n",
    "                # print(type(error).__name__,' : ', error)\n",
    "                continue\n",
    "\n",
    "            # Save List\n",
    "            data.append(data_normed_1.tolist())\n",
    "            label.append(label_1 - 1)\n",
    "            \n",
    "    # Zero-padding\n",
    "    data = zero_padding(data, T_MAX)\n",
    "\n",
    "    # Swap axes\n",
    "    data = np.swapaxes(np.swapaxes(data, 1, 3), 2, 3)   # [N,20,20',T_MAX]=>[N,T_MAX,20,20']\n",
    "    data = np.expand_dims(data, axis=2)    # [N,T_MAX,20,20]=>[N,T_MAX,20,20,1]\n",
    "\n",
    "    # Convert label to ndarray\n",
    "    label = np.array(label)\n",
    "\n",
    "    # data(ndarray): [N,T_MAX,20,20,1], label(ndarray): [N,N_MOTION]\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client 1 data extraction:\n",
      "\n",
      "Loaded dataset of 5875 samples, each sized (28, 1, 20, 20)\n",
      "\n",
      "client 2 data extraction:\n",
      "\n",
      "Loaded dataset of 6623 samples, each sized (29, 1, 20, 20)\n",
      "\n",
      "client 3 data extraction:\n",
      "\n",
      "Loaded dataset of 5374 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 4 data extraction:\n",
      "\n",
      "Loaded dataset of 625 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 5 data extraction:\n",
      "\n",
      "Loaded dataset of 1375 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 6 data extraction:\n",
      "\n",
      "Loaded dataset of 1124 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 7 data extraction:\n",
      "\n",
      "Loaded dataset of 624 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 8 data extraction:\n",
      "\n",
      "Loaded dataset of 623 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 9 data extraction:\n",
      "\n",
      "Loaded dataset of 624 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 10 data extraction:\n",
      "\n",
      "Loaded dataset of 875 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 11 data extraction:\n",
      "\n",
      "Loaded dataset of 875 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 12 data extraction:\n",
      "\n",
      "Loaded dataset of 875 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 13 data extraction:\n",
      "\n",
      "Loaded dataset of 875 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 14 data extraction:\n",
      "\n",
      "Loaded dataset of 875 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 15 data extraction:\n",
      "\n",
      "Loaded dataset of 874 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 16 data extraction:\n",
      "\n",
      "Loaded dataset of 875 samples, each sized (38, 1, 20, 20)\n",
      "\n",
      "client 17 data extraction:\n",
      "\n",
      "Loaded dataset of 875 samples, each sized (38, 1, 20, 20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# os.mkdir('./dataset')\n",
    "for user in range(1, 17+1):\n",
    "    print(f'client {user} data extraction:')\n",
    "    data, label = load_data(data_dir, ALL_MOTION, target_user=user)\n",
    "    torch.save({'data': data, 'label': label}, f'./dataset/user{user}.pt')\n",
    "    print('\\nLoaded dataset of ' + str(label.shape[0]) + ' samples, each sized ' + str(data[0,:,:].shape) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModule, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 10 * 10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "class ConvGRUModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, num_timestamps):\n",
    "        super(ConvGRUModel, self).__init__()\n",
    "        \n",
    "        # CNN module for each input timestamp\n",
    "        self.cnn_modules = nn.ModuleList([\n",
    "            CNNModule() for _ in range(num_timestamps)\n",
    "        ])\n",
    "        \n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(64, hidden_size, num_layers=num_timestamps, batch_first=True, dropout=0.25)\n",
    "\n",
    "        # Fully connected layer at the output of last GRU\n",
    "        self.fc_out = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # Relu activation for fully connected\n",
    "        self.relu = nn.ReLU()\n",
    "        # Softmax activation for classification\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply CNN module sequentially for each timestamp\n",
    "        x = [module(xi) for module, xi in zip(self.cnn_modules, x)]\n",
    "        x = torch.stack(x, dim=0)  # Stack along the time dimension\n",
    "\n",
    "        # GRU layer\n",
    "        x, _ = self.gru(x)\n",
    "\n",
    "        # Apply ReLU activation after the GRU layer\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Fully connected layer at the output of last GRU\n",
    "        x = self.fc_out(x[:, -1, :])\n",
    "        \n",
    "        # Softmax for classification\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])\n"
     ]
<<<<<<< HEAD
    }
   ],
   "source": [
    "model = ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX)\n",
    "# print(model)\n",
    "\n",
    "# Example input with batch size 16, 38 timestamps, height 20, width 20, and 1 channel\n",
    "input_data = torch.randn(1, 38, 1, 20, 20)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data)\n",
    "print(output.shape)  # This should print torch.Size([16, 10]) assuming 10 output classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "num_clients = 17\n",
    "client_datasets = {}\n",
    "client_loaders = {}\n",
    "\n",
    "for i in range(1, num_clients + 1):\n",
    "    # Load client data\n",
    "    client_data = torch.load(f'./dataset/user{i}.pt')\n",
    "    data = torch.from_numpy(client_data['data']).float()\n",
    "    label = torch.from_numpy(client_data['label']).long()\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    data_train, data_test, label_train, label_test = train_test_split(\n",
    "        data, label, test_size=fraction_for_test, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = TensorDataset(data_train, label_train)\n",
    "    test_dataset = TensorDataset(data_test, label_test)\n",
    "    client_datasets[f'client{i}'] = {'train': train_dataset, 'test':test_dataset}\n",
    "\n",
    "    # Set up data loaders for each client's\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    client_loaders[f'client{i}'] = {'train': train_loader, 'test':test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvGRUModel(\n",
       "  (cnn_modules): ModuleList(\n",
       "    (0): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (5): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (6): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (7): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (8): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (9): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (10): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (11): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (12): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (13): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (14): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (15): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (16): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (17): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (18): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (19): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (20): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (21): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (22): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (23): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (24): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (25): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (26): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (27): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (28): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (29): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (30): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (31): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (32): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (33): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (34): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (35): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (36): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (37): CNNModule(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): Flatten(start_dim=1, end_dim=-1)\n",
       "        (6): Linear(in_features=1600, out_features=64, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "        (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (10): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gru): GRU(64, 128, num_layers=38, batch_first=True, dropout=0.25)\n",
       "  (fc_out): Linear(in_features=128, out_features=9, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Determine if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.1661, Train Accuracy: 16.06%, Val Loss: 2.1614, Val Accuracy: 14.98%\n",
      "Epoch: 1, Train Loss: 2.1708, Train Accuracy: 18.18%, Val Loss: 2.1537, Val Accuracy: 20.60%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30  # Adjust based on your needs\n",
    "# Training loop with tracking of training and validation accuracies\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "best_val_accuracy = 0\n",
    "best_model_weights = None\n",
    "for epoch in range(num_epochs):\n",
    "    for client, train_loader in client_loaders.items():\n",
    "        user = int(client.split('client')[-1])\n",
    "        train_loss, train_accuracy = train(model, device, client_loaders[f'client{user}']['train'], optimizer , criterion)\n",
    "        val_loss, val_accuracy = test(model, device, client_loaders[f'client{user}']['test'] , criterion)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_weights = model.state_dict()\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')"
=======
    }
   ],
   "source": [
    "model = ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX)\n",
    "# print(model)\n",
    "\n",
    "# Example input with batch size 16, 38 timestamps, height 20, width 20, and 1 channel\n",
    "input_data = torch.randn(1, 38, 1, 20, 20)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data)\n",
    "print(output.shape)  # This should print torch.Size([16, 10]) assuming 10 output classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
>>>>>>> 5a4ddadbfc23c54b575afbc5eb78a417bef55ef3
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Load best model weights and evaluate on test set\n",
    "model.load_state_dict(best_model_weights)\n",
    "test_loss, test_accuracy = test(model, device, test_loader , criterion)\n",
    "\n",
    "# Plotting training and validation loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
=======
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 17\n",
    "client_datasets = {}\n",
    "\n",
    "# Create empty lists to store combined training and testing data and labels\n",
    "all_train_data = []\n",
    "all_train_labels = []\n",
    "all_test_data = []\n",
    "all_test_labels = []\n",
    "\n",
    "for i in range(1, num_clients + 1):\n",
    "    # Load client data\n",
    "    client_data = torch.load(f'./dataset/user{i}.pt')\n",
    "    data = torch.from_numpy(client_data['data']).float()\n",
    "    label = torch.from_numpy(client_data['label']).long()\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    data_train, data_test, label_train, label_test = train_test_split(\n",
    "        data, label, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Append the training and testing data and labels to the combined lists\n",
    "    all_train_data.append(data_train)\n",
    "    all_train_labels.append(label_train)\n",
    "    all_test_data.append(data_test)\n",
    "    all_test_labels.append(label_test)\n",
    "\n",
    "# Concatenate all training and testing data and labels\n",
    "all_train_data = torch.cat(all_train_data, dim=0)\n",
    "all_train_labels = torch.cat(all_train_labels, dim=0)\n",
    "all_test_data = torch.cat(all_test_data, dim=0)\n",
    "all_test_labels = torch.cat(all_test_labels, dim=0)\n",
    "\n",
    "# Create a centralized training dataset\n",
    "centralized_train_dataset = TensorDataset(all_train_data, all_train_labels)\n",
    "\n",
    "# Create a centralized testing dataset\n",
    "centralized_test_dataset = TensorDataset(all_test_data, all_test_labels)\n",
    "\n",
    "# Set up data loaders for centralized training and testing\n",
    "centralized_train_loader = DataLoader(centralized_train_dataset, batch_size=32, shuffle=True)\n",
    "centralized_test_loader = DataLoader(centralized_test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Determine if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20  # Adjust based on your needs\n",
    "# Training loop with tracking of training and validation accuracies\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "best_val_accuracy = 0\n",
    "best_model_weights = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, device, centralized_train_loader, optimizer , criterion)\n",
    "    val_loss, val_accuracy = test(model, device, centralized_test_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_weights = model.state_dict()\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fed average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "num_clients = 17\n",
    "client_datasets = {}\n",
    "client_loaders = {}\n",
    "\n",
    "for i in range(1, num_clients + 1):\n",
    "    # Load client data\n",
    "    client_data = torch.load(f'./dataset/user{i}.pt')\n",
    "    data = torch.from_numpy(client_data['data']).float()\n",
    "    label = torch.from_numpy(client_data['label']).long()\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    data_train, data_test, label_train, label_test = train_test_split(\n",
    "        data, label, test_size=fraction_for_test, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = TensorDataset(data_train, label_train)\n",
    "    test_dataset = TensorDataset(data_test, label_test)\n",
    "    client_datasets[f'client{i}'] = {'train': train_dataset, 'test':test_dataset}\n",
    "\n",
    "    # Set up data loaders for each client's\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    client_loaders[f'client{i}'] = {'train': train_loader, 'test':test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Determine if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.1668, Train Accuracy: 15.72%, Val Loss: 2.1608, Val Accuracy: 14.98%\n",
      "Epoch: 2, Train Loss: 2.1618, Train Accuracy: 15.47%, Val Loss: 2.1609, Val Accuracy: 14.98%\n",
      "Epoch: 3, Train Loss: 2.1619, Train Accuracy: 15.38%, Val Loss: 2.1611, Val Accuracy: 14.98%\n",
      "Epoch: 4, Train Loss: 2.1619, Train Accuracy: 15.23%, Val Loss: 2.1608, Val Accuracy: 14.98%\n",
      "Epoch: 5, Train Loss: 2.1618, Train Accuracy: 15.45%, Val Loss: 2.1597, Val Accuracy: 16.68%\n",
      "Epoch: 6, Train Loss: 2.1615, Train Accuracy: 14.94%, Val Loss: 2.1598, Val Accuracy: 14.98%\n",
      "Epoch: 7, Train Loss: 2.1615, Train Accuracy: 15.00%, Val Loss: 2.1604, Val Accuracy: 14.98%\n",
      "Epoch: 8, Train Loss: 2.1613, Train Accuracy: 15.70%, Val Loss: 2.1606, Val Accuracy: 14.98%\n",
      "Epoch: 9, Train Loss: 2.1614, Train Accuracy: 15.57%, Val Loss: 2.1595, Val Accuracy: 16.68%\n",
      "Epoch: 10, Train Loss: 2.1612, Train Accuracy: 15.09%, Val Loss: 2.1593, Val Accuracy: 16.00%\n",
      "Epoch: 11, Train Loss: 2.1616, Train Accuracy: 15.43%, Val Loss: 2.1594, Val Accuracy: 14.98%\n",
      "Epoch: 12, Train Loss: 2.1614, Train Accuracy: 15.91%, Val Loss: 2.1590, Val Accuracy: 16.68%\n",
      "Epoch: 13, Train Loss: 2.1611, Train Accuracy: 15.98%, Val Loss: 2.1634, Val Accuracy: 14.98%\n",
      "Epoch: 14, Train Loss: 2.1616, Train Accuracy: 15.60%, Val Loss: 2.1610, Val Accuracy: 14.98%\n",
      "Epoch: 15, Train Loss: 2.1617, Train Accuracy: 15.57%, Val Loss: 2.1601, Val Accuracy: 16.00%\n",
      "Epoch: 16, Train Loss: 2.1617, Train Accuracy: 14.98%, Val Loss: 2.1599, Val Accuracy: 14.98%\n",
      "Epoch: 17, Train Loss: 2.1614, Train Accuracy: 15.53%, Val Loss: 2.1595, Val Accuracy: 14.98%\n",
      "Epoch: 18, Train Loss: 2.1617, Train Accuracy: 15.04%, Val Loss: 2.1602, Val Accuracy: 16.00%\n",
      "Epoch: 19, Train Loss: 2.1614, Train Accuracy: 14.85%, Val Loss: 2.1599, Val Accuracy: 14.98%\n",
      "Epoch: 20, Train Loss: 2.1612, Train Accuracy: 15.00%, Val Loss: 2.1606, Val Accuracy: 14.98%\n",
      "Epoch: 21, Train Loss: 2.1609, Train Accuracy: 15.74%, Val Loss: 2.1590, Val Accuracy: 16.00%\n",
      "Epoch: 22, Train Loss: 2.1619, Train Accuracy: 15.68%, Val Loss: 2.1591, Val Accuracy: 16.68%\n",
      "Epoch: 23, Train Loss: 2.1615, Train Accuracy: 15.40%, Val Loss: 2.1607, Val Accuracy: 14.98%\n",
      "Epoch: 24, Train Loss: 2.1616, Train Accuracy: 15.36%, Val Loss: 2.1594, Val Accuracy: 14.98%\n",
      "Epoch: 25, Train Loss: 2.1614, Train Accuracy: 15.32%, Val Loss: 2.1600, Val Accuracy: 14.98%\n",
      "Epoch: 26, Train Loss: 2.1614, Train Accuracy: 14.68%, Val Loss: 2.1601, Val Accuracy: 16.00%\n",
      "Epoch: 27, Train Loss: 2.1621, Train Accuracy: 15.28%, Val Loss: 2.1591, Val Accuracy: 16.00%\n",
      "Epoch: 28, Train Loss: 2.1615, Train Accuracy: 15.45%, Val Loss: 2.1613, Val Accuracy: 14.98%\n",
      "Epoch: 29, Train Loss: 2.1613, Train Accuracy: 16.02%, Val Loss: 2.1600, Val Accuracy: 16.00%\n",
      "Epoch: 30, Train Loss: 2.1616, Train Accuracy: 15.45%, Val Loss: 2.1597, Val Accuracy: 16.00%\n",
      "Epoch: 31, Train Loss: 2.1615, Train Accuracy: 14.98%, Val Loss: 2.1590, Val Accuracy: 16.68%\n",
      "Epoch: 32, Train Loss: 2.1611, Train Accuracy: 15.91%, Val Loss: 2.1606, Val Accuracy: 14.98%\n",
      "Epoch: 33, Train Loss: 2.1614, Train Accuracy: 14.53%, Val Loss: 2.1596, Val Accuracy: 14.98%\n",
      "Epoch: 34, Train Loss: 2.1618, Train Accuracy: 15.57%, Val Loss: 2.1602, Val Accuracy: 14.98%\n",
      "Epoch: 35, Train Loss: 2.1616, Train Accuracy: 15.40%, Val Loss: 2.1596, Val Accuracy: 16.00%\n",
      "Epoch: 36, Train Loss: 2.1613, Train Accuracy: 15.87%, Val Loss: 2.1602, Val Accuracy: 14.98%\n",
      "Epoch: 37, Train Loss: 2.1613, Train Accuracy: 15.89%, Val Loss: 2.1604, Val Accuracy: 16.00%\n",
      "Epoch: 38, Train Loss: 2.1618, Train Accuracy: 15.15%, Val Loss: 2.1589, Val Accuracy: 16.68%\n",
      "Epoch: 39, Train Loss: 2.1616, Train Accuracy: 15.32%, Val Loss: 2.1597, Val Accuracy: 16.00%\n",
      "Epoch: 40, Train Loss: 2.1612, Train Accuracy: 15.21%, Val Loss: 2.1606, Val Accuracy: 14.98%\n",
      "Epoch: 41, Train Loss: 2.1619, Train Accuracy: 16.19%, Val Loss: 2.1600, Val Accuracy: 16.00%\n",
      "Epoch: 42, Train Loss: 2.1618, Train Accuracy: 14.96%, Val Loss: 2.1595, Val Accuracy: 14.98%\n",
      "Epoch: 43, Train Loss: 2.1614, Train Accuracy: 14.60%, Val Loss: 2.1613, Val Accuracy: 14.98%\n",
      "Epoch: 44, Train Loss: 2.1617, Train Accuracy: 15.06%, Val Loss: 2.1592, Val Accuracy: 16.00%\n",
      "Epoch: 45, Train Loss: 2.1608, Train Accuracy: 16.28%, Val Loss: 2.1615, Val Accuracy: 14.98%\n",
      "Epoch: 1, Train Loss: 2.1716, Train Accuracy: 18.06%, Val Loss: 2.1557, Val Accuracy: 20.60%\n",
      "Epoch: 2, Train Loss: 2.1694, Train Accuracy: 18.44%, Val Loss: 2.1548, Val Accuracy: 20.60%\n",
      "Epoch: 3, Train Loss: 2.1696, Train Accuracy: 18.44%, Val Loss: 2.1555, Val Accuracy: 20.60%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_14616/441539563.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_loaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'client{user}'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_loaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'client{user}'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_14616/1649372358.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(model, device, test_loader, criterion)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirhosein\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_14616/2322163352.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Apply CNN module sequentially for each timestamp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn_modules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Stack along the time dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_14616/2322163352.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Apply CNN module sequentially for each timestamp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn_modules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Stack along the time dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirhosein\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_14616/2322163352.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConvGRUModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirhosein\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirhosein\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirhosein\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirhosein\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirhosein\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 45  # Adjust based on your needs\n",
    "# Training loop with tracking of training and validation accuracies\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "best_val_accuracy = 0\n",
    "best_model_weights = None\n",
    "for client, train_loader in client_loaders.items():\n",
    "    user = int(client.split('client')[-1])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train(model, device, client_loaders[f'client{user}']['train'], optimizer , criterion)\n",
    "        val_loss, val_accuracy = test(model, device, client_loaders[f'client{user}']['test'] , criterion)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_weights = model.state_dict()\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model weights and evaluate on test set\n",
    "model.load_state_dict(best_model_weights)\n",
    "test_loss, test_accuracy = test(model, device, test_loader , criterion)\n",
    "\n",
    "# Plotting training and validation loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
>>>>>>> 5a4ddadbfc23c54b575afbc5eb78a417bef55ef3
    "plt.show()\n",
    "\n",
    "# Print best validation accuracy and test accuracy\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy:.2f}%')\n",
    "print(f'Test Accuracy of the final model: {test_accuracy:.2f}%')\n",
    "\n",
    "# Save the trained model if needed\n",
    "torch.save(model.state_dict(), './model/client{user}_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
