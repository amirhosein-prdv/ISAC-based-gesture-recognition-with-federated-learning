{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment for colab\n",
    "## upload datasetGenerator.py for preprocessing dataset\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# ! unzip -q \"/content/drive/MyDrive/Colab Notebooks/BVP.zip\"\n",
    "# ! python /content/datasetGenerator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_for_test = 0.2\n",
    "ALL_MOTION = [i for i in range(1, 10)]\n",
    "N_MOTION = len(ALL_MOTION) # Number of output classes\n",
    "T_MAX = 38 # Number of timestamps\n",
    "n_gru_hidden_units = 128\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModule, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=2, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8 * 10 * 10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "class ConvGRUModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, num_timestamps):\n",
    "        super(ConvGRUModel, self).__init__()\n",
    "        \n",
    "        # CNN module for each input timestamp\n",
    "        self.cnn_modules = nn.ModuleList([\n",
    "            CNNModule() for _ in range(num_timestamps)\n",
    "        ])\n",
    "        \n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(32, hidden_size, num_layers=num_timestamps, batch_first=True, dropout=0.25)\n",
    "\n",
    "        # Fully connected layer at the output of last GRU\n",
    "        self.fc_out = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # Relu activation for fully connected\n",
    "        self.relu = nn.ReLU()\n",
    "        # Softmax activation for classification\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply CNN module sequentially for each timestamp\n",
    "        x = np.swapaxes(x, 0, 1)\n",
    "        x = [module(xi) for module, xi in zip(self.cnn_modules, x)]\n",
    "        x = torch.stack(x, dim=1)  # Stack along the time dimension\n",
    "        \n",
    "        # GRU layer\n",
    "        x, _ = self.gru(x)\n",
    "\n",
    "        # Apply ReLU activation after the GRU layer\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Fully connected layer at the output of last GRU\n",
    "        x = self.fc_out(x[:, -1, :])\n",
    "        \n",
    "        # Softmax for classification\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "num_clients = 5\n",
    "batch_size = 128\n",
    "client_datasets = {}\n",
    "client_loaders = {}\n",
    "\n",
    "for i in range(1, num_clients + 1):\n",
    "    # Load client data\n",
    "    client_data = torch.load(f'./dataset/new/data{i}.pt')\n",
    "    data = torch.from_numpy(client_data['data']).float()\n",
    "    label = torch.from_numpy(client_data['label']).long()\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    data_train, data_test, label_train, label_test = train_test_split(\n",
    "        data, label, test_size=fraction_for_test, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = TensorDataset(data_train, label_train)\n",
    "    test_dataset = TensorDataset(data_test, label_test)\n",
    "    client_datasets[f'client{i}'] = {'train': train_dataset, 'test':test_dataset}\n",
    "\n",
    "    # Set up data loaders for each client's\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    client_loaders[f'client{i}'] = {'train': train_loader, 'test':test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedADMMAlgorithm:\n",
    "    def __init__(self, global_model, train_loader, rho):\n",
    "        self.global_model = global_model\n",
    "        self.rho = rho\n",
    "        self.train_loader = train_loader\n",
    "        self.num_clients = len(train_loader)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def admm_step(self, local_model, z, u):\n",
    "        # Update local model parameters using ADMM\n",
    "        for local_param, global_param, z_param, u_param in zip(local_model.parameters(),\n",
    "                                                               self.global_model.parameters(),\n",
    "                                                               z.parameters(),\n",
    "                                                               u.parameters()):\n",
    "            local_param.data = (global_param.data + z_param.data - u_param.data) / (2 * self.rho)\n",
    "\n",
    "    def train(self, model, device, train_loader, optimizer, criterion):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        return train_loss, train_accuracy\n",
    "\n",
    "    def test(self, model, device, test_loader, criterion):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        return test_loss, test_accuracy\n",
    "    \n",
    "    def run(self, num_rounds, num_epochs):\n",
    "        z = ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX).to(self.device)\n",
    "        u = ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX).to(self.device)\n",
    "\n",
    "        result = []\n",
    "        for round in range(num_rounds):\n",
    "            print(f\"---------- Round {round + 1}/{num_rounds} ----------\")\n",
    "\n",
    "            # List to store local model updates\n",
    "            local_model_updates = []\n",
    "            client_results = {'loss':[], 'accuracy':[]}\n",
    "\n",
    "            # Iterate over each client\n",
    "            for client_id in range(1, len(client_loaders)+1):\n",
    "                print(f\"\\nTraining on Client {client_id}\")\n",
    "\n",
    "                # Create a local copy\n",
    "                local_model = ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX).to(self.device)\n",
    "                local_model.load_state_dict(self.global_model.state_dict())  # Initialize with global model parameters\n",
    "\n",
    "                # Define loss function and optimizer\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.SGD(local_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "                # Local training\n",
    "                loss, accuracy = [], []\n",
    "                for epoch in range(num_epochs):\n",
    "\n",
    "                    train_loss, train_accuracy = self.train(local_model, self.device, client_loaders[f'client{client_id}']['train'], optimizer , criterion)\n",
    "                    val_loss, val_accuracy = self.test(local_model, self.device, client_loaders[f'client{client_id}']['test'], criterion)\n",
    "\n",
    "                    loss.append((train_loss, val_loss))\n",
    "                    accuracy.append((train_accuracy, val_accuracy))\n",
    "                    print(f'        Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "                    \n",
    "                local_model_updates.append(local_model.state_dict())\n",
    "                client_results['loss'].append(loss)\n",
    "                client_results['accuracy'].append(accuracy)\n",
    "\n",
    "                # Update local model parameters using ADMM\n",
    "                self.admm_step(local_model, z, u)\n",
    "\n",
    "                # Save the locally updated model parameters\n",
    "                local_model_updates.append(local_model.state_dict())\n",
    "\n",
    "            # Aggregate local model updates using FedADMM\n",
    "            averaged_state_dict = {}\n",
    "            for key in self.global_model.state_dict():\n",
    "                # Weighted average of the model parameters\n",
    "                averaged_state_dict[key] = sum(update[key] for update in local_model_updates) / len(local_model_updates)\n",
    "\n",
    "            # Update the global model with the aggregated parameters\n",
    "            self.global_model.load_state_dict(averaged_state_dict)\n",
    "            result.append(client_results)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Round 1/5 ----------\n",
      "\n",
      "Training on Client 1\n",
      "        Epoch: 1, Train Loss: 2.1968, Train Accuracy: 11.66%, Val Loss: 2.1967, Val Accuracy: 13.96%\n",
      "        Epoch: 2, Train Loss: 2.1967, Train Accuracy: 12.94%, Val Loss: 2.1965, Val Accuracy: 13.96%\n",
      "        Epoch: 3, Train Loss: 2.1965, Train Accuracy: 13.53%, Val Loss: 2.1964, Val Accuracy: 13.96%\n",
      "        Epoch: 4, Train Loss: 2.1964, Train Accuracy: 13.32%, Val Loss: 2.1962, Val Accuracy: 13.96%\n",
      "        Epoch: 5, Train Loss: 2.1962, Train Accuracy: 14.06%, Val Loss: 2.1961, Val Accuracy: 13.96%\n",
      "\n",
      "Training on Client 2\n",
      "        Epoch: 1, Train Loss: 2.1976, Train Accuracy: 8.21%, Val Loss: 2.1971, Val Accuracy: 8.91%\n",
      "        Epoch: 2, Train Loss: 2.1974, Train Accuracy: 12.06%, Val Loss: 2.1969, Val Accuracy: 20.60%\n",
      "        Epoch: 3, Train Loss: 2.1973, Train Accuracy: 15.76%, Val Loss: 2.1967, Val Accuracy: 20.60%\n",
      "        Epoch: 4, Train Loss: 2.1972, Train Accuracy: 17.95%, Val Loss: 2.1966, Val Accuracy: 20.60%\n",
      "        Epoch: 5, Train Loss: 2.1970, Train Accuracy: 18.25%, Val Loss: 2.1964, Val Accuracy: 20.60%\n",
      "\n",
      "Training on Client 3\n",
      "        Epoch: 1, Train Loss: 2.1979, Train Accuracy: 5.91%, Val Loss: 2.1979, Val Accuracy: 7.35%\n",
      "        Epoch: 2, Train Loss: 2.1978, Train Accuracy: 10.37%, Val Loss: 2.1977, Val Accuracy: 17.86%\n",
      "        Epoch: 3, Train Loss: 2.1977, Train Accuracy: 15.79%, Val Loss: 2.1976, Val Accuracy: 17.86%\n",
      "        Epoch: 4, Train Loss: 2.1975, Train Accuracy: 17.86%, Val Loss: 2.1975, Val Accuracy: 17.86%\n",
      "        Epoch: 5, Train Loss: 2.1974, Train Accuracy: 18.82%, Val Loss: 2.1974, Val Accuracy: 17.86%\n",
      "\n",
      "Training on Client 4\n",
      "        Epoch: 1, Train Loss: 2.1988, Train Accuracy: 3.23%, Val Loss: 2.1985, Val Accuracy: 2.90%\n",
      "        Epoch: 2, Train Loss: 2.1985, Train Accuracy: 5.43%, Val Loss: 2.1982, Val Accuracy: 15.82%\n",
      "        Epoch: 3, Train Loss: 2.1982, Train Accuracy: 10.29%, Val Loss: 2.1979, Val Accuracy: 15.82%\n",
      "        Epoch: 4, Train Loss: 2.1979, Train Accuracy: 13.29%, Val Loss: 2.1976, Val Accuracy: 15.82%\n",
      "        Epoch: 5, Train Loss: 2.1976, Train Accuracy: 14.54%, Val Loss: 2.1973, Val Accuracy: 15.82%\n",
      "\n",
      "Training on Client 5\n",
      "        Epoch: 1, Train Loss: 2.1990, Train Accuracy: 1.34%, Val Loss: 2.1988, Val Accuracy: 14.57%\n",
      "        Epoch: 2, Train Loss: 2.1986, Train Accuracy: 8.81%, Val Loss: 2.1985, Val Accuracy: 14.57%\n",
      "        Epoch: 3, Train Loss: 2.1983, Train Accuracy: 13.75%, Val Loss: 2.1981, Val Accuracy: 14.57%\n",
      "        Epoch: 4, Train Loss: 2.1979, Train Accuracy: 14.18%, Val Loss: 2.1978, Val Accuracy: 14.57%\n",
      "        Epoch: 5, Train Loss: 2.1976, Train Accuracy: 14.22%, Val Loss: 2.1975, Val Accuracy: 14.57%\n",
      "---------- Round 2/5 ----------\n",
      "\n",
      "Training on Client 1\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 15.45%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "\n",
      "Training on Client 2\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 12.80%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "\n",
      "Training on Client 3\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 11.63%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "\n",
      "Training on Client 4\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 16.94%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "\n",
      "Training on Client 5\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 14.40%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "---------- Round 3/5 ----------\n",
      "\n",
      "Training on Client 1\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "\n",
      "Training on Client 2\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "\n",
      "Training on Client 3\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "\n",
      "Training on Client 4\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "\n",
      "Training on Client 5\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "---------- Round 4/5 ----------\n",
      "\n",
      "Training on Client 1\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "\n",
      "Training on Client 2\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "\n",
      "Training on Client 3\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "\n",
      "Training on Client 4\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "\n",
      "Training on Client 5\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "---------- Round 5/5 ----------\n",
      "\n",
      "Training on Client 1\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 15.51%, Val Loss: nan, Val Accuracy: 16.68%\n",
      "\n",
      "Training on Client 2\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 12.78%, Val Loss: nan, Val Accuracy: 14.94%\n",
      "\n",
      "Training on Client 3\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 11.58%, Val Loss: nan, Val Accuracy: 11.72%\n",
      "\n",
      "Training on Client 4\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 17.22%, Val Loss: nan, Val Accuracy: 18.52%\n",
      "\n",
      "Training on Client 5\n",
      "        Epoch: 1, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 2, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 3, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 4, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n",
      "        Epoch: 5, Train Loss: nan, Train Accuracy: 14.54%, Val Loss: nan, Val Accuracy: 13.29%\n"
     ]
    }
   ],
   "source": [
    "fed_admm = FedADMMAlgorithm( global_model=ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX).to(device),\n",
    "                            train_loader=client_loaders,\n",
    "                            rho=0.01\n",
    "                            )\n",
    "\n",
    "fed_admm_result = fed_admm.run(num_rounds=5, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedADMMAlgorithm_:\n",
    "    def __init__(self, global_model, train_loader, rho):\n",
    "        self.train_loader = train_loader\n",
    "        self.num_devices = len(train_loader)\n",
    "        self.rho = rho\n",
    "        self.global_model = global_model\n",
    "        self.local_models = [ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX) for _ in range(self.num_devices)]\n",
    "        self.lagrange_multipliers = [torch.zeros_like(param) for param in self.global_model.parameters()]\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.global_model.parameters(), lr=0.01)\n",
    "\n",
    "    def train_local_models(self, dataloaders, num_local_epochs):\n",
    "        for i in range(self.num_devices):\n",
    "            self.local_models[i], _ = self._train_local_model(self.local_models[i], dataloaders[f'client{i+1}']['train'], num_local_epochs)\n",
    "\n",
    "    def _train_local_model(self, model, dataloader, num_local_epochs):\n",
    "        model.train()\n",
    "        for local_epoch in range(num_local_epochs):\n",
    "            for inputs, labels in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                local_loss = loss + (self.rho / 2) * torch.norm(torch.cat([param.flatten() for param in model.parameters()]) - self.lagrange_multipliers[i])**2\n",
    "                local_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        return model, local_loss.item()\n",
    "\n",
    "    def update_global_model(self):\n",
    "        self.global_model = self._update_global_model(self.local_models)\n",
    "\n",
    "    def _update_global_model(self, local_models):\n",
    "        global_model = ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX)\n",
    "        for global_param, local_param in zip(global_model.parameters(), zip(*[local_model.parameters() for local_model in local_models])):\n",
    "            global_param.data = torch.mean(torch.stack(local_param), dim=0)\n",
    "\n",
    "        return global_model\n",
    "\n",
    "    def update_lagrange_multipliers(self):\n",
    "        self.lagrange_multipliers = self._update_lagrange_multipliers(self.lagrange_multipliers, self.local_models, self.global_model)\n",
    "\n",
    "    def _update_lagrange_multipliers(self, lagrange_multipliers, local_models, global_model):\n",
    "        for i in range(len(lagrange_multipliers)):\n",
    "            lagrange_multipliers[i] += self.rho * (torch.cat([param.flatten() for param in local_models[i].parameters()]) - torch.cat([param.flatten() for param in global_model.parameters()]))\n",
    "\n",
    "        return lagrange_multipliers\n",
    "\n",
    "    def run(self, num_rounds, num_local_epochs):\n",
    "        for round_num in range(num_rounds):\n",
    "            print(f\"---------- Round {round_num + 1}/{num_rounds} ----------\")\n",
    "            self.train_local_models(self.train_loader, num_local_epochs)\n",
    "            self.update_global_model()\n",
    "            self.update_lagrange_multipliers()\n",
    "\n",
    "        return self.global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Round 1/2 ----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5870105) must match the size of tensor b (8) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_15260/4234252142.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m                                        \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclient_loaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      3\u001b[0m                                        rho=0.1)\n",
      "\u001b[1;32m----> 4\u001b[1;33m global_model = fedadmm_algorithm.run(num_rounds=2,\n",
      "\u001b[0m\u001b[0;32m      5\u001b[0m                                      num_local_epochs=5)\n",
      "\n",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_15260/3292764714.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, num_rounds, num_local_epochs)\u001b[0m\n",
      "\u001b[0;32m     51\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mround_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"---------- Round {round_num + 1}/{num_rounds} ----------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_local_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_local_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_global_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_lagrange_multipliers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_15260/3292764714.py\u001b[0m in \u001b[0;36mtrain_local_models\u001b[1;34m(self, dataloaders, num_local_epochs)\u001b[0m\n",
      "\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_local_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_local_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_devices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_local_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'client{i+1}'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_local_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train_local_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_local_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mC:\\Users\\AMIRHO~1\\AppData\\Local\\Temp/ipykernel_15260/3292764714.py\u001b[0m in \u001b[0;36m_train_local_model\u001b[1;34m(self, model, dataloader, num_local_epochs)\u001b[0m\n",
      "\u001b[0;32m     23\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     24\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0mlocal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlagrange_multipliers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26\u001b[0m                 \u001b[0mlocal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     27\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5870105) must match the size of tensor b (8) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "fedadmm_algorithm = FedADMMAlgorithm_(global_model=ConvGRUModel(n_gru_hidden_units, N_MOTION, T_MAX).to(device),\n",
    "                                       train_loader=client_loaders,\n",
    "                                       rho=0.1)\n",
    "global_model = fedadmm_algorithm.run(num_rounds=2,\n",
    "                                     num_local_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
